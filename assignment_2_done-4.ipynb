{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demostrate consequences of No Free Lunch Theorem: every model of machine learning makes **assumptions** about dataset properties. If the assumption fails, model perfoms badly. If the assumption is met, model perfoms good.\n",
    "\n",
    "For each of the tasks, described below:\n",
    "1. create a dataset to meet the description. For demonstration purposes, feature dimension size = 2    \n",
    "1. train the models and evaluate accuracy. (with a training pipeline, described in the seminar) \n",
    "1. Show which model is better.   \n",
    "1. Write what assumptions were used, and why model exibits such behavior.  \n",
    "1. Plot dataset to demonstrate its properties. (Scatter plot, classes with different colors)  \n",
    "\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Create dataset for binary classification task (binary crossentropy), \n",
    "where k-NN perfoms **worse** than logistic regression\n",
    "\n",
    "2. Create dataset for binary classification task (binary crossentropy), \n",
    "where logistic regression perfoms **worse** than k-NN\n",
    "\n",
    "3. Create dataset for binary classification task (binary crossentropy), \n",
    "where k-NN with cosine distance perfoms **better** than k-NN with euclidean distance\n",
    "\n",
    "\n",
    "! If there is no explanation, why one model perfoms better than the other on your dataset, the task does not count.  \n",
    "! if the training pipeline is not used, the task does not count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#all imports\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training_pipeline(classifier, X, y, param_dict):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"train score: \", classifier.score(X_train, y_train))\n",
    "    validator = GridSearchCV(classifier, param_grid = param_dict)\n",
    "    validator.fit(X_train, y_train)\n",
    "    print(\"best hyperparams: \", validator.best_params_)\n",
    "    classifier = validator.best_estimator_\n",
    "    print(\"test score: \", classifier.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#1. Create dataset for binary classification task (binary crossentropy), \n",
    "where k-NN perfoms **worse** than logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset creation\n",
    "X0 = [(x/10, (x/10)+1) for x in range(10)]\n",
    "y0 = [0] * len(X0)\n",
    "X1 = [(x, y-0.05) for x, y in X0]\n",
    "y1 = [1] * len(X1)\n",
    "X = np.array(X0+X1)\n",
    "y = np.array(y0+y1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter([i[0] for i in X0],\n",
    "            [i[1] for i in X0], color=\"red\")\n",
    "plt.scatter([i[0] for i in X1],\n",
    "            [i[1] for i in X1],  color=\"blue\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_pipeline(LogisticRegression(solver='liblinear'), X, y,\n",
    "                 {'C': np.arange(.01,1,.01)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_pipeline(KNeighborsClassifier(), X, y,\n",
    "                 {'n_neighbors': range(1, 10)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Объяснение поведения моделей\n",
    "Предположение о данных при использовании логистической регрессии заключается в том, что\n",
    "выборка линейно разделима в пространстве, т.е. один класс находится где-то в одной половине плокости,\n",
    "а другой - в другой.\n",
    "При использовании же метода $K$ ближайших соседей, как следует из названия, мы предполагаем, \n",
    "что объекты какого-либо класса в основном окружены объектами этого класса, т.е. что объекты \"скучкованы\"\n",
    "по классам.  \n",
    "Исходя из этого, мы можем построить выборки так, чтобы соответственно либо один, либо другой алгоритм\n",
    "классифакации работал хуже.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Принцип построения выборки для задания 1\n",
    "В данном случае, чтобы k-NN сработал хуже, нам нужно чтобы объекты одного класса находились по одну сторону\n",
    "какой-нибудь прямой, но в то же время близко к объектам другого класса.  \n",
    "Например, точку могут лежать на двух параллельных, но близких друг к другу прямых, так что на каждой\n",
    "прямой только один класс."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#2. Create dataset for binary classification task (binary crossentropy), \n",
    "where logistic regression perfoms **worse** than k-NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r1 =  [random.uniform(0,2) for i in range(100)]\n",
    "ang1 = [random.uniform(-np.pi, np.pi) for i in range(100)]\n",
    "r2 =  [random.uniform(5,5.5) for i in range(100)]\n",
    "ang2 = [random.uniform(-np.pi, np.pi) for i in range(100)]\n",
    "X_1 = [(r1[i]*np.cos(ang1[i]), r1[i]*np.sin(ang1[i])) for i in range(100)]\n",
    "y_1 = [1 for i in range(100)]\n",
    "X_2 = [(r2[i]*np.cos(ang2[i]), r2[i]*np.sin(ang2[i])) for i in range(100)]\n",
    "y_2 = [0 for i in range(100)]\n",
    "X = np.array(X_1+X_2)\n",
    "y = np.array(y_1+y_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter([i[0] for i in X_1],\n",
    "            [i[1] for i in X_1],\n",
    "            color=\"red\")\n",
    "plt.scatter([i[0] for i in X_2],\n",
    "            [i[1] for i in X_2], color=\"blue\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_pipeline(LogisticRegression(solver='liblinear'), X, y,\n",
    "                 {'C': np.arange(.01,1,.01)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_pipeline(KNeighborsClassifier(), X, y,\n",
    "                 {'n_neighbors': range(1, 10)})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Принцип построения выборки для задания 2\n",
    "Предположения, лежащие в основе каждого из алгоритмов классификации мы рассмотрели выше.\n",
    "В отличие от первого задания, теперь чтобы логистическая регрессия сработала хуже, нам нужно чтобы объекты одного класса\n",
    "были скучкованы в одну или более куч, но при этом чтобы классы, вернее их объекты, не лежали раздельно в двух полуплоскостях\n",
    "пространстве признаков, т.е. не были линейно разделимы.\n",
    "Вариантов такого разбиения много, например можно взять две концентрических окружности и вокруг них завести точки одного класса."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#3. Create dataset for binary classification task (binary crossentropy), \n",
    "where k-NN with cosine distance perfoms **better** than k-NN with euclidean distance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "## (1, 2) - направление вектора для класса 1:\n",
    "X_1 = [(1*k , 1*k) for k in np.random.normal(size=(100, ))]\n",
    "y_1 = [1 for i in range(100)]\n",
    "np.random.seed(42)\n",
    "## (0.95, 2.05) - направление вектора для класса 0:\n",
    "X_2 = [(1.05*k , 1.05*k ) for k in np.random.normal(size=(100, ))]\n",
    "y_2 = [0 for i in range(100)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter([i[0] for i in X_1],\n",
    "            [i[1] for i in X_1], color=\"red\")\n",
    "plt.scatter([i[0] for i in X_2],\n",
    "            [i[1] for i in X_2], color=\"blue\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_pipeline(KNeighborsClassifier(metric=\"euclidean\"), X, y,\n",
    "                 {'n_neighbors': range(1, 10)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_pipeline(KNeighborsClassifier(metric=\"cosine\"), X, y,\n",
    "                 {'n_neighbors': range(1, 10)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Принцип построения выборки для задания 3\n",
    "Уточним разницу в метриках для одного и того же алгоритма.  \n",
    "В случае евклидового расстояния, мы используем привычное нам расстояние между точками,\n",
    "в то время как косинусное расстояние учитывает только углы между векторами до точек из начала\n",
    "системы координат - то есть направления векторов. Так, все точки на одной прямой проходящей через начало\n",
    "системы координат будут иметь косинусное расстояние равно нулю между собой.  \n",
    "Воспользуемся этим свойством и разместим точки двух классов на двух разных прямых, проходящих\n",
    "через начало системы координат - в косинусном расстоянии они однозначно будут сгруппированы, в то время как\n",
    "в обычном евклидовом будут погрешности у их пересечения, около начала системы координат."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}